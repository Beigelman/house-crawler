# House Crawler ğŸ 

> Sistema automatizado de monitoramento de imÃ³veis que realiza web scraping em mÃºltiplos portais imobiliÃ¡rios, detecta novos anÃºncios e envia notificaÃ§Ãµes por email.

[![Deno](https://img.shields.io/badge/Deno-1.40+-000000?style=flat&logo=deno)](https://deno.land/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-3178C6?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Supabase](https://img.shields.io/badge/Supabase-Database-3ECF8E?style=flat&logo=supabase&logoColor=white)](https://supabase.com/)
[![Resend](https://img.shields.io/badge/Resend-Email-000000?style=flat)](https://resend.com/)

---

## ğŸ“‘ Ãndice

- [Sobre o Projeto](#-sobre-o-projeto)
- [Funcionalidades](#-funcionalidades)
- [Tecnologias Utilizadas](#ï¸-tecnologias-utilizadas)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [ConfiguraÃ§Ã£o](#ï¸-configuraÃ§Ã£o)
- [Uso](#-uso)
- [Arquitetura do Sistema](#ï¸-arquitetura-do-sistema)
- [Estrutura de Arquivos](#ï¸-estrutura-de-arquivos)
- [Estrutura da Tabela](#-estrutura-da-tabela-real_states)
- [Comandos Ãšteis](#-comandos-Ãºteis)
- [Personalizando os Crawlers](#-personalizando-os-crawlers)
- [AutomaÃ§Ã£o com Cron](#-automaÃ§Ã£o-com-cron)
- [Notas Importantes](#-notas-importantes)
- [Entendendo o robots.txt](#-entendendo-o-robotstxt)
- [SeguranÃ§a](#-seguranÃ§a)
- [Troubleshooting](#-troubleshooting)
- [Contribuindo](#-contribuindo)
- [Roadmap Futuro](#-roadmap-futuro)
- [LicenÃ§a](#-licenÃ§a)

---

## âš¡ Quick Start

```bash
# 1. Clone o repositÃ³rio
git clone <seu-repo>
cd house-crawler

# 2. Configure as variÃ¡veis de ambiente
cp .env.example .env
# Edite o .env com suas credenciais

# 3. Execute o crawler
deno task run
```

Para configuraÃ§Ã£o detalhada, veja a [seÃ§Ã£o de configuraÃ§Ã£o](#ï¸-configuraÃ§Ã£o).

---

## ğŸ“– Sobre o Projeto

O **House Crawler** Ã© uma ferramenta desenvolvida em TypeScript com Deno que automatiza a busca por imÃ³veis em sites de classificados. Ele foi projetado para coletar informaÃ§Ãµes de apartamentos/casas que atendem a critÃ©rios especÃ­ficos (localizaÃ§Ã£o, nÃºmero de quartos, valor, etc.) e notificar o usuÃ¡rio quando novos anÃºncios sÃ£o publicados.

### Caso de Uso

Ideal para quem estÃ¡ procurando imÃ³vel e quer ser notificado automaticamente quando novos anÃºncios aparecem nos portais, sem precisar verificar manualmente todos os dias.

### Como Funciona em Resumo

```
1. Crawlers acessam DF ImÃ³veis e Wimoveis
2. Extraem dados dos imÃ³veis (tÃ­tulo, valor, link)
3. Sincronizam com Supabase (sÃ³ novos sÃ£o inseridos)
4. Enviam email via Resend se houver novos imÃ³veis
```

## ğŸš€ Funcionalidades

- âœ… Coleta automÃ¡tica de imÃ³veis de diferentes sites:
  - **DF ImÃ³veis** (configurado para Asa Norte/Sul)
  - **Wimoveis** (configurado para BrasÃ­lia)
- ğŸ’¾ Armazenamento em banco de dados Supabase com detecÃ§Ã£o de duplicatas
- ğŸ“§ NotificaÃ§Ã£o por email com novos imÃ³veis encontrados (usando Resend)
- ğŸ”„ SincronizaÃ§Ã£o inteligente: apenas novos imÃ³veis sÃ£o inseridos
- ğŸ›¡ï¸ Sistema de detecÃ§Ã£o de duplicatas baseado no link do anÃºncio
- â±ï¸ Delay entre requisiÃ§Ãµes para evitar bloqueios (rate limiting)
- ğŸ“Š RelatÃ³rios detalhados de execuÃ§Ã£o no console

## ğŸ› ï¸ Tecnologias Utilizadas

- **[Deno](https://deno.land/)** - Runtime TypeScript/JavaScript moderno e seguro
- **[TypeScript](https://www.typescriptlang.org/)** - Linguagem com tipagem estÃ¡tica
- **[Cheerio](https://cheerio.js.org/)** - Biblioteca para parsing e manipulaÃ§Ã£o de HTML (jQuery-like)
- **[Supabase](https://supabase.com/)** - Backend-as-a-Service com PostgreSQL
- **[Resend](https://resend.com/)** - API moderna para envio de emails

### Por que Deno?

- âœ… TypeScript nativo (sem configuraÃ§Ã£o)
- âœ… PermissÃµes explÃ­citas (seguranÃ§a)
- âœ… Gerenciamento de dependÃªncias moderno (sem `node_modules`)
- âœ… Ferramentas integradas (formatter, linter, test runner)
- âœ… Suporte nativo a variÃ¡veis de ambiente

## ğŸ“‹ PrÃ©-requisitos

- [Deno](https://deno.land/) v1.40 ou superior
- Conta no [Supabase](https://supabase.com/) (gratuita)
- Conta no [Resend](https://resend.com/) (gratuita para atÃ© 100 emails/dia)

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. Configurar o Supabase

1. Crie um novo projeto no [Supabase](https://app.supabase.com/)
2. No SQL Editor do Supabase, execute o script `setup-supabase.sql` para criar a
   tabela
3. Obtenha suas credenciais em **Settings > API**:
   - Project URL
   - Anon/Public Key

### 2. Configurar o Resend

1. Crie uma conta em [Resend](https://resend.com/)
2. Obtenha sua API Key em **API Keys**
3. (Opcional) Configure um domÃ­nio personalizado em **Domains**

### 3. Configurar VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto com o seguinte conteÃºdo:

```bash
# Supabase Configuration
SUPABASE_URL=https://seu-projeto-id.supabase.co
SUPABASE_ANON_KEY=sua-chave-anon-aqui

# Email Configuration (Resend)
RESEND_API_KEY=re_sua_chave_api_aqui

# Email de origem (remetente)
# Para testes, use: onboarding@resend.dev
# Para produÃ§Ã£o, use seu domÃ­nio verificado
FROM_EMAIL=imoveis@seudominio.com

# Emails de destino (destinatÃ¡rios separados por vÃ­rgula)
TO_EMAILS=seu.email@example.com,outro.email@example.com
```

**âœ¨ O arquivo `.env` Ã© carregado automaticamente!** NÃ£o Ã© necessÃ¡rio exportar
as variÃ¡veis manualmente.

**Importante:**

- O arquivo `.env` jÃ¡ estÃ¡ no `.gitignore` e nÃ£o serÃ¡ commitado
- `TO_EMAILS` aceita mÃºltiplos emails separados por vÃ­rgula
- Para testes, use `onboarding@resend.dev` como `FROM_EMAIL`

### 4. Instalar DependÃªncias

As dependÃªncias serÃ£o instaladas automaticamente pelo Deno ao executar o
projeto.

## ğŸ¯ Uso

### Testar ConfiguraÃ§Ã£o de Email

Antes de executar o crawler completo, teste a configuraÃ§Ã£o de email:

```bash
deno task test-email
```

### Executar o Crawler

```bash
deno task run
```

Ou diretamente:

```bash
deno run --allow-net --allow-write --allow-env src/main.ts
```

### O que acontece ao executar?

1. ğŸ” Coleta imÃ³veis de todos os sites configurados
2. ğŸ’¾ Salva backup local em `imoveis.json`
3. â˜ï¸ Sincroniza com Supabase:
   - Novos imÃ³veis sÃ£o inseridos
   - ImÃ³veis jÃ¡ existentes (mesmo link) sÃ£o ignorados
4. ğŸ“§ Envia email de notificaÃ§Ã£o com os novos imÃ³veis (se houver)
5. ğŸ“Š Exibe relatÃ³rio com nÃºmero de imÃ³veis novos e status do email

## ğŸ—ï¸ Arquitetura do Sistema

### Fluxo de Dados

```mermaid
graph LR
    A[main.ts] --> B[df_imoveis.ts]
    A --> C[wimoveis.ts]
    B --> D[utils.ts]
    C --> D
    B --> E[Cheerio/HTML Parsing]
    C --> E
    A --> F[supabase.ts]
    F --> G[Supabase Cloud]
    A --> H[email.ts]
    H --> I[Resend API]
    H --> J[email-template.ts]
```

### Processo de ExecuÃ§Ã£o

1. **InicializaÃ§Ã£o** (`main.ts`)
   - Carrega variÃ¡veis de ambiente do arquivo `.env`
   - Inicia o processo de coleta

2. **Coleta de Dados** (Crawlers)
   - `df_imoveis.ts`: Acessa a pÃ¡gina de listagem do DF ImÃ³veis
   - `wimoveis.ts`: Acessa a pÃ¡gina de listagem do Wimoveis
   - Para cada site:
     - Extrai links de todos os imÃ³veis da listagem
     - Acessa cada link individualmente
     - Faz parsing do HTML usando Cheerio
     - Extrai tÃ­tulo, valor e link do imÃ³vel
     - Aguarda 1.2s entre requisiÃ§Ãµes (rate limiting)

3. **SincronizaÃ§Ã£o com Banco de Dados** (`supabase.ts`)
   - Consulta quais imÃ³veis jÃ¡ existem (baseado no link)
   - Filtra apenas os novos imÃ³veis
   - Insere apenas os que ainda nÃ£o estÃ£o no banco

4. **NotificaÃ§Ã£o** (`email.ts`)
   - Se houver novos imÃ³veis:
     - Gera email HTML e texto usando template
     - Envia via Resend para os destinatÃ¡rios configurados

## ğŸ—‚ï¸ Estrutura de Arquivos

```
house-crawler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.ts              # ğŸ¯ Ponto de entrada e orquestraÃ§Ã£o
â”‚   â”œâ”€â”€ df_imoveis.ts        # ğŸ¢ Crawler para DF ImÃ³veis
â”‚   â”œâ”€â”€ wimoveis.ts          # ğŸ¢ Crawler para Wimoveis
â”‚   â”œâ”€â”€ supabase.ts          # ğŸ’¾ IntegraÃ§Ã£o com Supabase
â”‚   â”œâ”€â”€ email.ts             # ğŸ“§ Envio de emails via Resend
â”‚   â”œâ”€â”€ email-template.ts    # ğŸ“ Template HTML/texto dos emails
â”‚   â”œâ”€â”€ robots.ts            # ğŸ¤– VerificaÃ§Ã£o de robots.txt
â”‚   â”œâ”€â”€ types.ts             # ğŸ“ DefiniÃ§Ãµes de tipos TypeScript
â”‚   â”œâ”€â”€ utils.ts             # ğŸ› ï¸ FunÃ§Ãµes utilitÃ¡rias
â”‚   â”œâ”€â”€ database.types.ts    # ğŸ—„ï¸ Tipos gerados do Supabase
â”‚   â”œâ”€â”€ test-connection.ts   # ğŸ§ª Teste de conexÃ£o Supabase
â”‚   â”œâ”€â”€ test-email.ts        # ğŸ§ª Teste de envio de email
â”‚   â””â”€â”€ test-robots.ts       # ğŸ§ª Teste de robots.txt
â”œâ”€â”€ deno.json                # âš™ï¸ ConfiguraÃ§Ã£o Deno e tasks
â”œâ”€â”€ setup-supabase.sql       # ğŸ—ƒï¸ Script de criaÃ§Ã£o da tabela
â””â”€â”€ .env                     # ğŸ” VariÃ¡veis de ambiente (criar)
```

### DescriÃ§Ã£o dos MÃ³dulos

#### ğŸ“Œ `main.ts` - Orquestrador Principal
Ponto de entrada do sistema. Coordena a execuÃ§Ã£o de todas as etapas:
- Carrega configuraÃ§Ãµes do `.env`
- Chama os crawlers sequencialmente
- Agrega os resultados
- Sincroniza com Supabase
- Envia notificaÃ§Ãµes por email

#### ğŸ•·ï¸ `df_imoveis.ts` e `wimoveis.ts` - Crawlers
Cada crawler segue o mesmo padrÃ£o:
1. **`collectListingLinks()`**: Extrai todos os links de imÃ³veis da pÃ¡gina de listagem
2. **`parseProperty()`**: Acessa cada link e extrai os dados do imÃ³vel
3. **`extract**()`**: FunÃ§Ãµes especÃ­ficas para extrair tÃ­tulo, preÃ§o, etc.

**EstratÃ©gias de Parsing:**
- Usam Cheerio para navegaÃ§Ã£o no DOM HTML
- Seletores CSS customizados para cada site
- NormalizaÃ§Ã£o de texto (remoÃ§Ã£o de espaÃ§os extras)
- ConstruÃ§Ã£o de URLs absolutas
- ValidaÃ§Ã£o de domÃ­nio

#### ğŸ’¾ `supabase.ts` - Gerenciamento de Dados
- **`getSupabaseClient()`**: Cria e retorna instÃ¢ncia do cliente Supabase (singleton)
- **`insertNewProperties()`**: LÃ³gica de inserÃ§Ã£o inteligente
  - Consulta links existentes no banco
  - Filtra apenas novos imÃ³veis
  - Insere em batch

**EstratÃ©gia de DeduplicaÃ§Ã£o:**
O campo `link` Ã© a chave primÃ¡ria, garantindo que o mesmo imÃ³vel nÃ£o seja inserido duas vezes.

#### ğŸ“§ `email.ts` - Sistema de NotificaÃ§Ãµes
- ValidaÃ§Ã£o de configuraÃ§Ã£o (API key, emails)
- GeraÃ§Ã£o de conteÃºdo HTML e texto
- Envio via Resend API
- Suporte a mÃºltiplos destinatÃ¡rios
- Tratamento de erros robusto

#### ğŸ¨ `email-template.ts` - Templates de Email
- **`generateEmailHTML()`**: Template HTML responsivo
- **`generateEmailText()`**: VersÃ£o texto simples
- FormataÃ§Ã£o de valores, links e informaÃ§Ãµes

#### ğŸ› ï¸ `utils.ts` - UtilitÃ¡rios
- **`fetchDocument()`**: Busca e faz parsing de pÃ¡ginas HTML
- **`normalizeWhitespace()`**: Remove espaÃ§os extras e quebras de linha
- **`buildAbsoluteUrl()`**: ConstrÃ³i URLs absolutas a partir de relativas
- **`isSameDomain()`**: Valida se URL pertence ao domÃ­nio esperado
- **`printProperty()`**: Exibe imÃ³vel no console formatado

#### ğŸ¤– `robots.ts` - VerificaÃ§Ã£o de robots.txt
MÃ³dulo opcional para web scraping Ã©tico:
- **`fetchRobotsTxt()`**: Baixa o robots.txt de um site
- **`parseRobotsTxt()`**: Faz parsing do conteÃºdo
- **`isUrlAllowed()`**: Verifica se URL Ã© permitida
- **`checkUrlAgainstRobotsTxt()`**: FunÃ§Ã£o auxiliar completa

**Nota:** Este mÃ³dulo Ã© educacional. O erro 403 que vocÃª recebe vem de um firewall/WAF, nÃ£o do robots.txt. As URLs atuais do crawler **respeitam** o robots.txt de ambos os sites.

#### ğŸ“ `types.ts` - Tipos
```typescript
interface Property {
  titulo: string;  // TÃ­tulo/descriÃ§Ã£o do imÃ³vel
  valor: string;   // Valor (formato texto)
  link: string;    // URL do anÃºncio (chave primÃ¡ria)
}
```

## ğŸ“Š Estrutura da Tabela `real_states`

| Coluna     | Tipo      | DescriÃ§Ã£o                       |
| ---------- | --------- | ------------------------------- |
| link       | TEXT (PK) | URL do anÃºncio (chave primÃ¡ria) |
| titulo     | TEXT      | TÃ­tulo do anÃºncio               |
| valor      | TEXT      | Valor do imÃ³vel                 |
| created_at | TIMESTAMP | Data da primeira coleta         |

## ğŸ”§ Comandos Ãšteis

```bash
# Testar conexÃ£o com Supabase
deno task test

# Testar configuraÃ§Ã£o de email
deno task test-email

# Verificar robots.txt dos sites
deno task test-robots

# Executar o crawler
deno task run

# Formatar cÃ³digo
deno task fmt

# Lint
deno task lint
```

## ğŸ¨ Personalizando os Crawlers

### Modificar CritÃ©rios de Busca

Os critÃ©rios de busca estÃ£o definidos nas URLs de listagem de cada crawler:

**DF ImÃ³veis** (`src/df_imoveis.ts`):
```typescript
const LIST_URL =
  "https://www.dfimoveis.com.br/venda/df/brasilia/asa-norte,asa-sul/imoveis/3,4-quartos?suites=1&vagasdegaragem=1&valorfinal=1200000&areainicial=90";
```

**Wimoveis** (`src/wimoveis.ts`):
```typescript
const LIST_URL =
  "https://www.wimoveis.com.br/venda/apartamentos/brasil/desde-3-ate-4-quartos/areac-elevador?areaUnit=1&bathroom=2&coveredArea=90,&loc=Z:42705,42704&price=,1200000";
```

VocÃª pode ajustar os parÃ¢metros diretamente na URL ou usar o site para fazer uma busca e copiar a URL resultante.

### Adicionar Novos Sites

Para adicionar um novo site de imÃ³veis:

1. Crie um novo arquivo (ex: `src/novo_site.ts`)
2. Implemente as funÃ§Ãµes:
   ```typescript
   async function collectListingLinks(listUrl: string): Promise<string[]>
   async function parseProperty(url: string): Promise<Property>
   export async function collectNovoSiteProperties(): Promise<Property[]>
   ```
3. Use os seletores CSS apropriados para extrair os dados
4. Adicione no `main.ts`:
   ```typescript
   import { collectNovoSiteProperties } from "./novo_site.ts";
   const novoSiteProperties = await collectNovoSiteProperties();
   ```

### Ajustar Rate Limiting

O delay entre requisiÃ§Ãµes estÃ¡ configurado em 1.2 segundos. Para ajustar:

```typescript
await delay(1200); // Altere o valor em milissegundos
```

## ğŸ”„ AutomaÃ§Ã£o com Cron

Para executar o crawler automaticamente em intervalos regulares:

### Linux/macOS (crontab)

```bash
# Editar crontab
crontab -e

# Executar todo dia Ã s 9h e 18h
0 9,18 * * * cd /caminho/para/house-crawler && /caminho/para/deno task run >> /tmp/house-crawler.log 2>&1
```

### GitHub Actions

âš ï¸ **LimitaÃ§Ã£o**: Alguns sites podem bloquear requisiÃ§Ãµes do GitHub Actions (erro 403). O crawler continuarÃ¡ funcionando com os sites que nÃ£o bloquearem.

Crie `.github/workflows/crawler.yml`:

```yaml
name: House Crawler
on:
  schedule:
    - cron: '0 9,18 * * *'  # 9h e 18h UTC
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: denoland/setup-deno@v1
        with:
          deno-version: v2.x
      - name: Run crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          RESEND_API_KEY: ${{ secrets.RESEND_API_KEY }}
          FROM_EMAIL: ${{ secrets.FROM_EMAIL }}
          TO_EMAILS: ${{ secrets.TO_EMAILS }}
        run: deno task run
```

**Configurar Secrets no GitHub:**
1. VÃ¡ em Settings > Secrets and variables > Actions
2. Adicione cada variÃ¡vel de ambiente como um secret

## ğŸ“ Notas Importantes

### Funcionamento
- A chave primÃ¡ria Ã© o `link`, garantindo que nÃ£o haja duplicatas
- ImÃ³veis jÃ¡ cadastrados sÃ£o automaticamente ignorados
- O campo `created_at` registra quando o imÃ³vel foi visto pela primeira vez
- Emails sÃ£o enviados apenas quando hÃ¡ novos imÃ³veis encontrados
- O template de email Ã© responsivo e funciona em todos os clientes de email

### Tratamento de Erros
- Se houver erro ao coletar um imÃ³vel especÃ­fico, ele Ã© registrado no console e o crawler continua
- Se houver erro ao sincronizar com Supabase, o processo Ã© interrompido
- Se houver erro ao enviar email, o processo Ã© interrompido

### Performance
- Delay de 1.2s entre requisiÃ§Ãµes evita bloqueios por rate limiting
- InserÃ§Ã£o em batch no Supabase (mais eficiente)
- Consulta prÃ©via de links existentes minimiza operaÃ§Ãµes desnecessÃ¡rias

### Web Scraping Ã‰tico
- âœ… Respeita robots.txt dos sites
- âœ… Implementa delays entre requisiÃ§Ãµes (1.2s)
- âœ… Usa User-Agent identificÃ¡vel (navegador real)
- âœ… NÃ£o sobrecarrega os servidores
- âœ… Acessa apenas pÃ¡ginas pÃºblicas de listagem
- âš ï¸ Sites podem alterar sua estrutura HTML - nesse caso, os seletores CSS precisarÃ£o ser atualizados

**VerificaÃ§Ã£o do robots.txt:**
```bash
# Verificar se suas URLs respeitam o robots.txt
deno task test-robots
```

**Status atual:**
- âœ… DF ImÃ³veis: Nossas URLs sÃ£o permitidas
- âœ… Wimoveis: Nossas URLs sÃ£o permitidas
- âš ï¸ O erro 403 vem de firewall/WAF, nÃ£o do robots.txt

## ğŸ¤– Entendendo o robots.txt

### O Que Ã‰?

O `robots.txt` Ã© um arquivo na raiz de sites (`https://exemplo.com/robots.txt`) que indica quais partes do site podem ser acessadas por bots/crawlers. Ã‰ um "acordo de cavalheiros" da web.

### Como Funciona?

```
User-agent: *           # Aplica-se a todos os bots
Disallow: /admin/       # ProÃ­be acesso a /admin/
Disallow: /api/         # ProÃ­be acesso a /api/
Allow: /api/public/     # Permite /api/public/ (exceÃ§Ã£o)
```

### Status dos Sites Monitorados

**DF ImÃ³veis:**
```
âœ… Permitido: /venda/... (pÃ¡ginas pÃºblicas de listagem)
âœ… Permitido: /imovel/... (pÃ¡ginas de detalhes)
âŒ Bloqueado: /favoritos/, /conta/, /visitas/ (Ã¡reas privadas)
```

**Wimoveis:**
```
âœ… Permitido: PÃ¡ginas de listagem com filtros
âŒ Bloqueado: ?sort=*, ?page=>5, tracking, APIs internas
```

### Nosso Crawler Respeita?

**Sim!** âœ… Todas as URLs que acessamos sÃ£o permitidas pelos respectivos `robots.txt`.

VocÃª pode verificar com:
```bash
deno task test-robots
```

### EntÃ£o Por Que o Erro 403?

O `robots.txt` Ã© **opcional** e **informativo**. Sites podem:
- Ter robots.txt E proteÃ§Ã£o adicional (firewall/WAF)
- Bloquear IPs de data centers (como GitHub Actions)
- Usar proteÃ§Ã£o anti-bot (Cloudflare, etc.)

**O erro 403 vem de um firewall/WAF, nÃ£o do robots.txt.**

### Leitura Adicional

- [RFC 9309 - Robots Exclusion Protocol](https://www.rfc-editor.org/rfc/rfc9309.html)
- [Google Search Central - robots.txt](https://developers.google.com/search/docs/crawling-indexing/robots/intro)

---

## ğŸ”’ SeguranÃ§a

### Boas PrÃ¡ticas Implementadas

- âœ… VariÃ¡veis de ambiente em arquivo `.env` (nÃ£o commitado)
- âœ… Uso de chave `ANON_KEY` para operaÃ§Ãµes pÃºblicas
- âœ… PolÃ­ticas RLS (Row Level Security) no Supabase
- âœ… ValidaÃ§Ã£o de dados antes de inserÃ§Ã£o
- âœ… Tratamento adequado de erros

### RecomendaÃ§Ãµes Adicionais

- ğŸ” Configure polÃ­ticas RLS personalizadas no Supabase conforme necessÃ¡rio
- ğŸ”‘ Nunca commit suas chaves de API no repositÃ³rio
- ğŸ”„ Rotacione suas chaves periodicamente
- ğŸ“§ Use domÃ­nio verificado no Resend para evitar spam

## ğŸ› Troubleshooting

### Erro: "VariÃ¡veis de ambiente nÃ£o encontradas"

**SoluÃ§Ã£o**: Verifique se o arquivo `.env` existe na raiz do projeto e contÃ©m todas as variÃ¡veis necessÃ¡rias.

### âš ï¸ Erro 403 Forbidden no GitHub Actions

**Problema**: O crawler funciona localmente mas falha no GitHub Actions com erro `403 Forbidden`.

**Causa**: Sites imobiliÃ¡rios bloqueiam requisiÃ§Ãµes vindas de IPs de data centers (como os do GitHub Actions) para se proteger de bots.

**SoluÃ§Ãµes:**

#### 1ï¸âƒ£ SoluÃ§Ã£o Recomendada: Aceitar a LimitaÃ§Ã£o
O cÃ³digo jÃ¡ estÃ¡ preparado para lidar com isso:
- Se um site retornar 403, ele serÃ¡ pulado
- O crawler continuarÃ¡ com os outros sites
- VocÃª receberÃ¡ email apenas dos sites que funcionaram

#### 2ï¸âƒ£ Alternativa: Rodar Localmente com Cron
Execute o crawler em sua prÃ³pria mÃ¡quina ao invÃ©s do GitHub Actions:

```bash
# Linux/macOS - Edite o crontab
crontab -e

# Adicione (executar 2x por dia):
0 9,18 * * * cd /caminho/para/house-crawler && deno task run >> /tmp/crawler.log 2>&1
```

#### 3ï¸âƒ£ Alternativa: Usar VPS/Servidor PrÃ³prio
Deploy em um servidor VPS (DigitalOcean, AWS, etc.):
- IPs residenciais tÃªm menos chance de bloqueio
- Mais controle sobre o ambiente
- Pode usar proxies se necessÃ¡rio

#### 4ï¸âƒ£ Alternativa AvanÃ§ada: Usar Proxy (Pago)
Adicione suporte a proxies residenciais nos crawlers:
```typescript
const PROXY_URL = Deno.env.get("PROXY_URL");
// Configurar proxy nas requisiÃ§Ãµes
```

**Nota**: Web scraping deve respeitar os termos de uso dos sites. O erro 403 Ã© a forma do site indicar que nÃ£o quer ser acessado automaticamente.

### Erro ao coletar imÃ³veis (HTTP 404)

**Causa**: A URL de listagem mudou ou nÃ£o existe mais.

**SoluÃ§Ã£o**: 
1. Verifique se a URL de listagem ainda estÃ¡ vÃ¡lida
2. Acesse o site manualmente e copie a nova URL
3. Atualize a constante `LIST_URL` no crawler correspondente

### Emails nÃ£o estÃ£o sendo enviados

**VerificaÃ§Ãµes**:
1. Execute `deno task test-email` para testar a configuraÃ§Ã£o
2. Verifique se `RESEND_API_KEY` estÃ¡ correta
3. Confirme que `TO_EMAILS` estÃ¡ no formato correto
4. Se usar domÃ­nio personalizado, verifique se estÃ¡ verificado no Resend

### ImÃ³veis duplicados no banco

**Causa**: ImproÃ¡vel, pois o `link` Ã© chave primÃ¡ria.

**SoluÃ§Ã£o**: Verifique se a funÃ§Ã£o `insertNewProperties` estÃ¡ sendo chamada corretamente.

### Performance lenta

**PossÃ­veis causas**:
- Muitos imÃ³veis na listagem
- LatÃªncia de rede alta
- Rate limiting dos sites

**SoluÃ§Ãµes**:
- Aumente o delay entre requisiÃ§Ãµes
- Filtre melhor os critÃ©rios de busca
- Execute em horÃ¡rios de menor trÃ¡fego

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para:

1. **Adicionar novos sites**: Crie crawlers para outros portais imobiliÃ¡rios
2. **Melhorar parsers**: Otimizar a extraÃ§Ã£o de dados
3. **Adicionar recursos**: Filtros avanÃ§ados, mais campos de dados, etc.
4. **Reportar bugs**: Abra uma issue descrevendo o problema
5. **Melhorar documentaÃ§Ã£o**: CorreÃ§Ãµes e esclarecimentos

### Como Contribuir

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“Š Roadmap Futuro

PossÃ­veis melhorias planejadas:

- [ ] Suporte a mais portais imobiliÃ¡rios (OLX, VivaReal, Imovelweb)
- [ ] Interface web para configuraÃ§Ã£o
- [ ] Filtros avanÃ§ados (distÃ¢ncia de pontos de interesse, etc.)
- [ ] AnÃ¡lise de preÃ§os e alertas de oportunidades
- [ ] HistÃ³rico de variaÃ§Ã£o de preÃ§os
- [ ] IntegraÃ§Ã£o com Telegram/WhatsApp
- [ ] Dashboard com estatÃ­sticas
- [ ] DetecÃ§Ã£o de imÃ³veis removidos

## ğŸ“„ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto e estÃ¡ disponÃ­vel sob a licenÃ§a MIT.

---

## ğŸ“ Suporte

Se vocÃª encontrou algum problema ou tem sugestÃµes:

- ğŸ› Abra uma [issue](../../issues) no GitHub
- ğŸ’¡ Compartilhe ideias de melhorias
- â­ DÃª uma estrela se este projeto foi Ãºtil!

---

**Desenvolvido com â¤ï¸ usando Deno e TypeScript**

*Ãšltima atualizaÃ§Ã£o: Outubro 2025*
